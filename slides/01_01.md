---
title: What is GitHub Copilot?
slide: '<!-- .slide: data-visibility="hidden"-->'
---

<!-- .slide: data-state="layout-title" class="bg-dark"-->

# What is GitHub Copilot?

> >

Let's talk about what GitHub Copilot is and what makes it special.

---

# GitHub Copilot

- Code assistant
- Based on GPT-4
- Focused LLM
- Popular languages

> >

At it's core, GitHub Copilot is a code assistant, to help you write code faster.

It's based on GPT-4, the model from Open AI that powers Chat GPT, the most popular web application in history which gathered over 100 million users in the first 2 months.

This is a special customized version of GPT-4, which is a Large Language Model. It has been trained on massive amounts of publicly available text, in the case of Copilot, gigabytes of code.


Because it's trained on public code, the more popular the language, the more code available, the better recommendations, so it's best for popular languages like Python, JavaScript and Ruby.

---

# Large Language Model

- Models predict outcomes
- LLMs focus on language
- Code is language

> >

Models are formulas used to predict events. One example you might be familiar with are hurricane models that attempt to chart the paths of storms every year.

LLMs try to predict what should come next in a sequence of words. It's like the auto complete that happens when you use a search engine. It will give you  a suggestion that may or may be what you're looking for.

A programming language is a pretty simple language. There's grammar and rules. Unlike a lot of languages, it doesn't have things like slang and words with changing meanings.

---

# How LLMs Work

- Tokenizes data
- Vocabulary size
- Token cost

> >

Let's dig a bit more specifically as to how LLMs work.

LLMs, in their training phase, absorb all of the available data and converts words, characters and other symbols to numbers known as tokens, because they're easier for computers to work with.

The tokens become the vocabulary of the language, so more complex languages have more complex vocabularies.

When you make requests from the LLM, the size of the language as well as the amount of information you're providing can affect the cost of using the model, both in terms of compute power as well as procesing usage.

---

# Training Phases

- Predict next token/word
- Mimics human communication
- Reinforced learning

> > 

After the tokenization, the models go through training phases where...given different inputs, they try to determining the probability of what the next piece of code or token should be. They do this continually until they finish the message that they write back to you.

This mimmics the way people communicate in that we often put together sentences by thinking of what the next word should be. You probably notice it more when you can't find the right word and perhaps someone else suggests it.

Most of the training is a statistical analysis of the tokens that determines a most likely next token, but that's followed by human led reinforcement, the model's predictions get better over time, as it is trained on what humans prefer.

By looking at gigabytes of code, it's able to determine the answer to a problem that is most likely to be correct answer for the question you asked.